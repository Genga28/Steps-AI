{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()   \n",
    "url = \"https://docs.nvidia.com/cuda/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Function to scrape a webpage and its sub-links recursively\n",
    "def scrape_page(url, depth, max_depth, visited_urls, data):\n",
    "    if depth > max_depth or url in visited_urls:\n",
    "        return\n",
    "\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract content (modify as per your requirement)\n",
    "        content = soup.get_text()\n",
    "        data.append((url, content))\n",
    "\n",
    "        # Extract links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            full_link = urljoin(url, link['href'])\n",
    "            if 'https://docs.nvidia.com/cuda/' in full_link:\n",
    "                scrape_page(full_link, depth + 1, max_depth, visited_urls, data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Main function to initiate scraping\n",
    "def main():\n",
    "    start_url = \"https://docs.nvidia.com/cuda/\"\n",
    "    max_depth = 2  # Depth of sub-links to scrape\n",
    "    visited_urls = set()\n",
    "    scraped_data = []\n",
    "\n",
    "    scrape_page(start_url, 0, max_depth, visited_urls, scraped_data)\n",
    "\n",
    "    # Print some sample scraped data\n",
    "    for url, content in scraped_data[:5]:  # Displaying first 5 entries for illustration\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Content:\\n{content[:200]}...\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (687263809.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [8], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip3 install sentence-transformers\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pip install selenium webdriver-manager\n",
    "#pip install selenium webdriver-manager requests beautifulsoup4 sentence-transformers milvus transformers flask\n",
    "#python your_script.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming scraped_data is populated from previous step\n",
    "# Simplified chunking (each paragraph as a chunk)\n",
    "def chunk_data(data):\n",
    "    chunks = []\n",
    "    for url, content in data:\n",
    "        paragraphs = content.split('\\n\\n')  # Split by paragraphs\n",
    "        chunks.extend([(url, para.strip()) for para in paragraphs if para.strip()])\n",
    "    return chunks\n",
    "\n",
    "# Chunk the scraped data\n",
    "chunked_data = chunk_data(scraped_data)\n",
    "\n",
    "# Displaying first few chunks\n",
    "for i, (url, chunk) in enumerate(chunked_data[:5]):\n",
    "    print(f\"Chunk {i + 1}: URL: {url}\\n{chunk[:200]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from milvus import Milvus, DataType\n",
    "\n",
    "# Initialize Milvus client\n",
    "client = Milvus(uri='tcp://127.0.0.1:19530')\n",
    "\n",
    "# Create a collection\n",
    "collection_name = 'cuda_docs'\n",
    "collection_param = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"embeddings\", \"type\": DataType.FLOAT_VECTOR, \"params\": {\"dim\": 768}},\n",
    "        {\"name\": \"metadata\", \"type\": DataType.VARCHAR, \"params\": {\"max_length\": 1024}}\n",
    "    ],\n",
    "    \"segment_row_limit\": 4096,\n",
    "    \"auto_id\": True\n",
    "}\n",
    "client.create_collection(collection_name, collection_param)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to encode chunks into embeddings and insert into Milvus\n",
    "def encode_and_insert_chunks(chunks):\n",
    "    for url, chunk in chunks:\n",
    "        embeddings = model.encode([chunk], show_progress_bar=False)[0]\n",
    "        embeddings = embeddings.tolist()  # Convert numpy array to list\n",
    "\n",
    "        # Insert into Milvus\n",
    "        client.insert(collection_name, [{\"embeddings\": embeddings, \"metadata\": {\"url\": url, \"chunk\": chunk}}])\n",
    "\n",
    "# Encode and insert chunks into Milvus\n",
    "encode_and_insert_chunks(chunked_data)\n",
    "\n",
    "# Flush changes to disk\n",
    "client.flush([collection_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_endpoint():\n",
    "    data = request.json\n",
    "    query = data.get('query', '')\n",
    "    retrieved_docs = retrieve(query)\n",
    "    answer = answer_question(query, retrieved_docs)\n",
    "    return jsonify(answer)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
